[
  {
    "objectID": "posts/quarto-website/index.html",
    "href": "posts/quarto-website/index.html",
    "title": "Crafting your website with Quarto and GitHub",
    "section": "",
    "text": "pwd lists current directory;\nls lists directory content. For example, run ls my-dir to know what is inside the directory called my-dir.\ncd stands for change directory. To navigate to a directory, use: cd   &lt;path-to-directory&gt;;\nmv Moves or renames files. Usage: mv &lt;old-name/location&gt; &lt;new-name/location&gt;;"
  },
  {
    "objectID": "posts/quarto-website/index.html#terminal-basics",
    "href": "posts/quarto-website/index.html#terminal-basics",
    "title": "Crafting your website with Quarto and GitHub",
    "section": "",
    "text": "pwd lists current directory;\nls lists directory content. For example, run ls my-dir to know what is inside the directory called my-dir.\ncd stands for change directory. To navigate to a directory, use: cd   &lt;path-to-directory&gt;;\nmv Moves or renames files. Usage: mv &lt;old-name/location&gt; &lt;new-name/location&gt;;"
  },
  {
    "objectID": "posts/quarto-website/index.html#pre-requisites",
    "href": "posts/quarto-website/index.html#pre-requisites",
    "title": "Crafting your website with Quarto and GitHub",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nWe assume you have:\n\nQuarto installed on your labtop;\nA GitHub account;\nWe also assume you are able to connect to GitHub using SSH keys. For more details, see this\nA terminal available (Windows users, see this)."
  },
  {
    "objectID": "posts/quarto-website/index.html#step-1-create-a-repo",
    "href": "posts/quarto-website/index.html#step-1-create-a-repo",
    "title": "Crafting your website with Quarto and GitHub",
    "section": "Step 1: create a repo",
    "text": "Step 1: create a repo\nIn this workshop, we assume you are creating a personal web page. GitHub allows you to host a personal website for free under the domain &lt;username&gt;.github.io. We first need to create a GitHub repository named like that. For example, in this talk, we will create a website for a user called statcomp-org (see Figure 1).\n\n\n\nFigure 1: Screenshot of the page where repos are created"
  },
  {
    "objectID": "posts/quarto-website/index.html#step-2-using-our-quarto-template",
    "href": "posts/quarto-website/index.html#step-2-using-our-quarto-template",
    "title": "Crafting your website with Quarto and GitHub",
    "section": "Step 2: Using our quarto template",
    "text": "Step 2: Using our quarto template\nOpen a terminal window and navigate to a directory where you want to keep your website. On my laptop, I have a directory (or folder) called git-projects where I store all the git repos I work with. The following code chunk creates a directory to store the files we will use to create your website locally and navigates to it.\n\n\nCode\nmkdir &lt;your-username&gt;.github.io\ncd &lt;your-username&gt;.github.io\n\n\nNow, we obtain a template running the following command:\n\n\nCode\nquarto use template swosc/quarto-template\n\n\nYou will be asked the following questions. Type “Y” and the key “enter” and “.” and the key “enter”, respectively.\n\n\nCode\nQuarto templates may execute code when documents are rendered. If you do not \ntrust the authors of the template, we recommend that you do not install or \nuse the template.\n? Do you trust the authors of this template (Y/n) › Y\n? Directory name: › .\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen running the command below, we are assuming your current “working directory” in the terminal is the directory where you are going to keep your website. Run pwd, to make sure you are working on the right directory.\n\n\nLet us use ls to track the files that were generated in our directory:\n\n\nCode\nls\nLICENSE.md         blog.qmd           highlight-author.R profile.jpg\nMakefile           custom.scss        index.qmd          pubs.bib\n_quarto.yml        github             lua-refs.lua       styles.css\napa-cv.csl         gitignore          posts\n\n\n\n\n\n\n\n\nTip\n\n\n\nAlways avoid creating filenames with special charaters or spaces.\n\n\nThe structure of the folder is as follows:\n\n\nCode\n.\n├── LICENSE.md\n├── Makefile\n├── _quarto.yml\n├── apa-cv.csl\n├── blog.qmd\n├── custom.scss\n├── github\n│   └── workflows\n│       └── publish.yml\n├── gitignore\n├── highlight-author.R\n├── index.qmd\n├── lua-refs.lua\n├── posts\n│   ├── _metadata.yml\n│   ├── post-with-code\n│   │   ├── image.jpg\n│   │   └── index.qmd\n│   └── welcome\n│       ├── index.qmd\n│       └── thumbnail.jpg\n├── profile.jpg\n├── pubs.bib\n└── styles.css\n\n6 directories, 19 files\n\n\nSee below a description on what each of those files are:\n\nLICENSE.md contains the license for your website repo. Currently, the license in the template is a CC-BY license. That is, anyone is free to share and adapt, must give appropriate credit. You can use the usethis package to change this license. See this link for thurther info.\nMakefile will only be used if you wish to include a page in the website with your publications.\n_quarto.yml is perhaps the most important file we will keep. It will give instructions on how to design the website (more on this later).\napa-cv.csl is a helper that controls the templates of the publications list that may be generated using the Makefile.\nblog.md is the file that will control how your “blog” page looks like.\ncustom.scss helps to further customize the website.\ngithub is a folder that we need to rename to .github. It will help GitHub to publish your website online.\ngitignore rename to .gitignore. This file will store the name of the files you do not want to push to GitHub.\nhighlight-author.R is a helper R script intended to help with the creation of a opublications’ list (optional).\nindex.qmd is the landing page of your website.\nlua-refs.lua is a lua script that will be used to generate your publications list (optional).\nposts/ is a folder containing your posts and some metadata (optional, only if you want to blog as well)\n\nThe _metadata.yml contains some information on how to deal with your posts. In particular, it will tell GitHub avoid compiling them (we will compile the posts locally)\nEach subfolder represents a blog post. Within the blogpost folder, we need a index.qmd file (more on this later).\n\nprofile.jpg replace this with your profile picture.\npubs.bibs (optional) input your publications in BibTeX here.\nstyles.css allows for further customization of the website."
  },
  {
    "objectID": "posts/quarto-website/index.html#step-3-setting-up-git-repo",
    "href": "posts/quarto-website/index.html#step-3-setting-up-git-repo",
    "title": "Crafting your website with Quarto and GitHub",
    "section": "Step 3: setting up git repo",
    "text": "Step 3: setting up git repo\nThe next step is to setup the git repo. Before doing so, let’s rename the .github and .gitignore files. This is achieved by running:\n\n\nCode\nmv github .github\nmv gitignore .gitignore\n\n\nNow, let’s set the git repo and push the gitignore file to the remote repo.\n\n\nCode\ngit init\ngit add .gitignore\ngit commit -m \"commiting gitignore\"\ngit branch -M main\ngit remote add origin git@github.com:&lt;your-username&gt;/&lt;your-username&gt;.github.io\ngit push -u origin main\n\n\nIf you want to get a grasp of how the website is looking like, run:\n\n\nCode\nquarto preview\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can use quarto preview to visualize the changes anytime you modify something on your quarto website."
  },
  {
    "objectID": "posts/quarto-website/index.html#step-4-lets-taylor-our-website",
    "href": "posts/quarto-website/index.html#step-4-lets-taylor-our-website",
    "title": "Crafting your website with Quarto and GitHub",
    "section": "Step 4: Let’s taylor our website!",
    "text": "Step 4: Let’s taylor our website!\n\n\n\n\n\n\nNote\n\n\n\nYou may use your preferred IDE to edit .qmd files. Popular choices are Rstudio, VScode, and (not so popular) emacs. For the last two, we advise you to install the appropriate extensions to enjoy all the capabilities of quarto.\n\n\nFirst, we will customize the index.qmd file.\nAt the beginning, the index.qmd file will look like this:\n---\nimage: profile.jpg\nabout:\n  template: jolla\n  links:\n    - icon: twitter\n      text: Twitter\n      href: https://twitter.com\n    - icon: linkedin\n      text: LinkedIn\n      href: https://linkedin.com\n    - icon: github\n      text: Github\n      href: https://github.com\n\n---\n\n&lt;Write somethign about yourself or your website&gt;\nIn general, we want to replace the href field under Twiter, Linkedin, and GitHub with the link to our own accounts. The profile.jpg file can also be changed (or we can just overwrite it).\nThere are 5 possible inputs for the template field. They are:\n\njolla (used in the provided template)\ntrestles\nsolana\nmarquee\nbroadside\n\nPlay around with the different options to figure out which one suits you best.\nWe can include more pages and resources to our website by adding new .qmd (or even .pdf) files to it and editing the _quarto.yml file.\nFor example, assume we want to include our CV as a .pdf file. First, we copy our .pdf file into the directory where our website project is located, let’s say we named it cv.pdf. Next, we add the following to our _quarto.yml file.\n---\nproject:\n  type: website\n  output-dir: docs\n  render:\n    - \"*.qmd\"\n\nwebsite:\n  title: \"How do you wanna call your website?\"\n  search: true\n  cookie-consent: false\n  twitter-card: true\n  open-graph: true\n  site-url: https://&lt;yourwebsite&gt;.github.io\n  navbar:\n    right:\n      - blog.qmd\n      - text: \"How do I wanna call my CV\"\n        href: cv.pdf\n      - icon: github\n        href: https://github.com/\n      - icon: twitter\n        href: https://twitter.com\n  page-footer: \n    left: \"You can write somethig here too.\"\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n---\nAdding a new .qmd file is even simpler. We do not need the text and href parameters. First, create a .qmd file. Let us call it my-new-qmd.qmd. The “body” of the file can be anything you want, but the header should have (at least) a title. The title will tell how this file will appear in the navbar.\n---\ntitle: \"My new `.qmd` file\"\n---\nTo add my-new-qmd.qmd to the navbar, edit the _quarto.yml as follows:\n---\nproject:\n  type: website\n  output-dir: docs\n  render:\n    - \"*.qmd\"\n\nwebsite:\n  title: \"How do you wanna call your website?\"\n  search: true\n  cookie-consent: false\n  twitter-card: true\n  open-graph: true\n  site-url: https://&lt;yourwebsite&gt;.github.io\n  navbar:\n    right:\n      - blog.qmd\n      - text: \"How do I wanna call my CV\"\n        href: cv.pdf\n      - my-new-qmd.qmd\n      - icon: github\n        href: https://github.com/\n      - icon: twitter\n        href: https://twitter.com\n  page-footer: \n    left: \"You can write somethig here too.\"\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n---\nWe can further customize the navbar. For example, we can move the twitter and github icons to the right and everything else to the left.\n---\nproject:\n  type: website\n  output-dir: docs\n  render:\n    - \"*.qmd\"\n\nwebsite:\n  title: \"How do you wanna call your website?\"\n  search: true\n  cookie-consent: false\n  twitter-card: true\n  open-graph: true\n  site-url: https://&lt;yourwebsite&gt;.github.io\n  navbar:\n    left:\n      - blog.qmd\n      - text: \"How do I wanna call my CV\"\n        href: cv.pdf\n        - my-new-qmd.qmd\n    right:\n      - icon: github\n        href: https://github.com/\n      - icon: twitter\n        href: https://twitter.com\n  page-footer: \n    left: \"You can write somethig here too.\"\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n---\nWe can easily change the way our website look by changing the theme parameter under format and html. To get a list of the 25 themes available, take a look at this link.\nAlternatively, we can change the colors of a theme using a scss (or css file). For instance, let’s add the university colors to the cosmo theme. First, create a file called custom.scss in the root directory of our website. Next, add the following lines to the file\n#| eval: false\n/*-- scss:defaults --*/\n         \n$primary:       #000E2F !default;\n$secondary:     #E4002B !default;\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to include the /*-- scss:defaults --*/ to the custom.scss file. Otherwise, the changes will not be applied.\n\n\nFinally, we rewrite our _quarto.yml file as follows:\n---\nproject:\n  type: website\n  output-dir: docs\n  render:\n    - \"*.qmd\"\n\nwebsite:\n  title: \"How do you wanna call your website?\"\n  search: true\n  cookie-consent: false\n  twitter-card: true\n  open-graph: true\n  site-url: https://&lt;yourwebsite&gt;.github.io\n  navbar:\n    left:\n      - blog.qmd\n      - text: \"How do I wanna call my CV\"\n        href: cv.pdf\n        - my-new-qmd.qmd\n    right:\n      - icon: github\n        href: https://github.com/\n      - icon: twitter\n        href: https://twitter.com\n  page-footer: \n    left: \"You can write somethig here too.\"\n\nformat:\n  html:\n    theme:\n      - cosmo\n      - custom.scss\n    css: styles.css\n---\nWe can also import google fonts into our website. To test it, let’s change the font to Ubuntu. First, we use the Google Font API to import the desired font (for further details see this).\n#| eval: false\n/*-- scss:defaults --*/\n\n// colors\n$primary:       #000E2F !default;\n$secondary:     #E4002B !default;\n\n// fonts\n@import url('https://fonts.googleapis.com/css2?family=Ubuntu&display=swap');\nNext, we update, once again, the _quarto.yml file.\n---\nproject:\n  type: website\n  output-dir: docs\n  render:\n    - \"*.qmd\"\n\nwebsite:\n  title: \"How do you wanna call your website?\"\n  search: true\n  cookie-consent: false\n  twitter-card: true\n  open-graph: true\n  site-url: https://&lt;yourwebsite&gt;.github.io\n  navbar:\n    left:\n      - blog.qmd\n      - text: \"How do I wanna call my CV\"\n        href: cv.pdf\n        - my-new-qmd.qmd\n    right:\n      - icon: github\n        href: https://github.com/\n      - icon: twitter\n        href: https://twitter.com\n  page-footer: \n    left: \"You can write somethig here too.\"\n\nformat:\n  html:\n    theme:\n      - cosmo\n      - custom.scss\n    mainfont: Ubuntu\n    css: styles.css\n---\n\n\n\n\n\n\nTip\n\n\n\nThe possibilities of customization are endless! My advice is to keep it simple.\n\n\n\nBonus\nIf you have your publications in a .bib file, we can use some lua and R to get a tab with your publications working. If you have no idea about what BibTeX is, take a look at this link.\nKalu (Figure 2 (a)) has her publications information in a file called pubs.bib. She has published one paper as a first author and another one as a co-author, collaborating with Cacau (Figure 2 (b)).\n\n\n\n\n\n\n\n(a) Kalu\n\n\n\n\n\n\n\n(b) Cacau\n\n\n\n\nFigure 2: Some of my pets.\n\n\nSee below how her pubs.bib file looks like\n@article{hernandes2023meowmazing,\n  title =        {Meowmazing paper I wrote},\n  author =       {Kalu Hernandez},\n  journal =      {Meow journal},\n  number =       000,\n  pages =        {0--0},\n  year =         2023,\n  publisher =    {Crazy cats dot com}\n}\n\n@article{godoy2022growlmazing,\n  title =        {Growlmazing paper we wrote},\n  author =       {Cacau Godoy, Kalu Hernandez},\n  journal =      {Meow journal},\n  number =       000,\n  pages =        {0--0},\n  year =         2023,\n  publisher =    {Dogs and cats getting along}\n}\nTo transform this pubs.bib into a .qmd file, we will use make (for more details see this). There are some helper files in our directory, they are Makefile, apa-cv.csv (citation style file), highlight-author.R, and lua-refs.lua. The Makefile looks as follows:\nall: publications\n    @quarto render\n\npublications: pubs.bib apa-cv.csl lua-refs.lua\n    @quarto pandoc -L lua-refs.lua \\\n        pubs.bib --csl=apa-cv.csl \\\n        -V --toc=false \\\n        --to=markdown-citations \\\n        -o publications.qmd\n    @Rscript highlight-author.R \\\n        \"Hernandez, K.\" \"publications.qmd\"\n\nclean:\n    rm -rf publications* *~\nIt uses the lua-refs.lua script to create a publications.qmd file based on pubs.bib. Next, it uses R to highlight a selected author.\n\n\n\n\n\n\nImportant\n\n\n\nTo make it work with your .bib file, you need to populate pubs.bib with your publications or include another .bib file in the directory. If that’s the case, replace pubs.bib by &lt;your-file&gt;.bib in the Makefile. Make sure you also changes Hernandez, K. in the Makefile to the way your name appears in the references.\n\n\nNow, we run:\n#| eval: false\nmake publications\nand it will generate a publications.qmd file. Now we only need to tell our _quarto.yml how to “find” this new file."
  },
  {
    "objectID": "posts/quarto-website/index.html#step-4-publishing",
    "href": "posts/quarto-website/index.html#step-4-publishing",
    "title": "Crafting your website with Quarto and GitHub",
    "section": "Step 4: Publishing",
    "text": "Step 4: Publishing\nPublishing the website will be one of the easiest parts. We will use GitHub Actions (GA) and GitHub pages.\nBasically, all you have to do is to go to your repository on GitHub, click on Settings &gt; Pages. Next, under Build and Deployment, set Deploy from a branch, and, under Branch, select main and /docs.\n\nNext, you should create a new branch called gh-pages. To do so, click on main, and then on View all branches. Next, create new branch. Name the new branch as gh-pages.\nNow, every time you push your changes to github, the website will be deployed (published) at &lt;your-username&gt;.github.io."
  },
  {
    "objectID": "posts/quarto-website/index.html#closing-comments",
    "href": "posts/quarto-website/index.html#closing-comments",
    "title": "Crafting your website with Quarto and GitHub",
    "section": "Closing comments",
    "text": "Closing comments\nCreating a website is a fun task that may take a lot of your time if you are too attached to details, be mindful. The best way to move forward, is being good at Googling and paying attention to the error messages (do never dismiss them!).\nThere are many resources online teaching you how to customize your documents, write reports, and so on in Quarto. If you want to do something different, is very likely that someone has already done that. So, once again, always search the web thoroughly to avoid “reinventing the wheel”."
  },
  {
    "objectID": "posts/quarto-website/index.html#further-resources-explore",
    "href": "posts/quarto-website/index.html#further-resources-explore",
    "title": "Crafting your website with Quarto and GitHub",
    "section": "Further resources (explore)",
    "text": "Further resources (explore)\n\nQuarto website provides a bunch of resources;\nCustomizing Quarto Websites talk, by Sam Csik;"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Presentations",
    "section": "",
    "text": "Getting to know Stan - SWOSC\n\n\n\n\n\n\n\nbayesian inference\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nBen Stockton\n\n\n\n\n\n\n  \n\n\n\n\nCrafting your website with Quarto and GitHub\n\n\n\n\n\n\n\nmiscelllaneous\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2023\n\n\nLucas Godoy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Student Workshop Series on Computing",
    "section": "",
    "text": "Series of talks and workshops from students to students."
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html",
    "href": "posts/intro-to-stan/intro_to_stan.html",
    "title": "Getting to know Stan - SWOSC",
    "section": "",
    "text": "To get started, we’ll head over to Stan’s documentation to see how to get set up in R. For this presentation, I’ll use the CmdStan toolchain that’s implemented in R by the cmdstanr package (Gabry, Češnovar, and Johnson 2023). There are also Python, command line, Matlab, Julia, and Stata interfaces to Stan and a Python interface for cmdstan called CmdStanPy (“Cmdstanpy  Python Interface to CmdStan  CmdStanPy 1.1.0 Documentation,” n.d.).\n\n\nCode\n# install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\nlibrary(cmdstanr)\n\n\nThis is cmdstanr version 0.6.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/lcgodoy/.cmdstan/cmdstan-2.33.1\n\n\n- CmdStan version: 2.33.1\n\n\nCode\n# cmdstanr::install_cmdstan()\ncmdstanr::cmdstan_version()\n\n\n[1] \"2.33.1\"\n\n\nCode\ncmdstanr::cmdstan_path()\n\n\n[1] \"/Users/lcgodoy/.cmdstan/cmdstan-2.33.1\"\n\n\nCode\ncmdstanr::check_cmdstan_toolchain()\n\n\nThe C++ toolchain required for CmdStan is setup properly!"
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#installation",
    "href": "posts/intro-to-stan/intro_to_stan.html#installation",
    "title": "Getting to know Stan - SWOSC",
    "section": "",
    "text": "To get started, we’ll head over to Stan’s documentation to see how to get set up in R. For this presentation, I’ll use the CmdStan toolchain that’s implemented in R by the cmdstanr package (Gabry, Češnovar, and Johnson 2023). There are also Python, command line, Matlab, Julia, and Stata interfaces to Stan and a Python interface for cmdstan called CmdStanPy (“Cmdstanpy  Python Interface to CmdStan  CmdStanPy 1.1.0 Documentation,” n.d.).\n\n\nCode\n# install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\nlibrary(cmdstanr)\n\n\nThis is cmdstanr version 0.6.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/lcgodoy/.cmdstan/cmdstan-2.33.1\n\n\n- CmdStan version: 2.33.1\n\n\nCode\n# cmdstanr::install_cmdstan()\ncmdstanr::cmdstan_version()\n\n\n[1] \"2.33.1\"\n\n\nCode\ncmdstanr::cmdstan_path()\n\n\n[1] \"/Users/lcgodoy/.cmdstan/cmdstan-2.33.1\"\n\n\nCode\ncmdstanr::check_cmdstan_toolchain()\n\n\nThe C++ toolchain required for CmdStan is setup properly!"
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#a-very-brief-introduction-to-bayesian-data-analysis",
    "href": "posts/intro-to-stan/intro_to_stan.html#a-very-brief-introduction-to-bayesian-data-analysis",
    "title": "Getting to know Stan - SWOSC",
    "section": "A Very Brief Introduction to Bayesian Data Analysis",
    "text": "A Very Brief Introduction to Bayesian Data Analysis\nThe broadest conceptual overview of Bayesian data analysis is that this methodology allows us to incorporate prior knowledge about the model/data into our analysis which is based on a posterior distribution that is derived from the prior and likelihood. Bayesian inference treats the parameters of the model as random variables whose distribution we are interested in either deriving analytically or approximating through computation. This is a key distinction from frequentist methods taught in math stat and applied stat where parameters are fixed values and their estimators are functions of a random sample and the sampling distribution of the estimator is used for inference.\n\n\n\n\n\n\nBayes Rule\n\n\n\nA quick reminder of Bayes Rule.\nLet \\(\\theta\\) be a random variable with (prior) distribution \\(p(\\theta)\\), \\(Y\\) be a random variable that depends on \\(\\theta\\) with conditional distribution or likelihood \\(p(y | \\theta)\\). Then their joint distribution is \\(p(y, \\theta) = p(y|\\theta) p(\\theta)\\).\nBayes rule lets us flip the conditioning from the likelihood to get \\(p(\\theta | y)\\)\n\\[\np(\\theta | y) = \\frac{p(y, \\theta)}{p(y)} = \\frac{p(y|\\theta) p(\\theta)}{\\int p(y, \\theta) d\\theta} \\approx p(y|\\theta) p(\\theta)\n\\]\n\n\nTo be a little more specific, we have three central components to the model. For ease of exposition, let’s consider the simple regression model \\(E(Y_i | X_i = x_i, \\boldsymbol{\\beta}, \\sigma^2) = \\beta_0 + \\beta_1 X_i\\) and \\(Var(Y_i | X_i = x_i, \\boldsymbol{\\beta}, \\sigma^2) = \\sigma^2\\) where we have \\(i = 1,\\dots,N\\) observations of \\((Y_i, X_i)'\\) from Ch. 14 of Bayesian Data Analysis (Gelman et al. 2013, 354–58). From here on I will suppress conditioning on the observed predictor \\(X_i = x_i\\) since we are considering the design matrix \\(X = (1_N, \\mathbf{x})\\) to be fixed and known where \\(\\mathbf{x} = (x_1,\\dots, x_N)'\\).\n\nThe Prior: a distribution for the parameters that doesn’t depend on the data.\n\nFor the regression model, we have three parameters \\(\\beta_0, \\beta_1, \\sigma^2\\). We assign a probability distribution \\(p(\\beta_0, \\beta_1, \\sigma^2) \\equiv p(\\boldsymbol{\\beta}, \\sigma^2)\\) to these parameters. One common way to do this are a non-informative prior so that the prior is uniform \\(p(\\boldsymbol{\\beta}, \\log\\sigma) = 1 \\equiv p(\\boldsymbol{\\beta}, \\sigma^2) \\propto \\sigma^{-2}\\).\n\nThe (Data) Likelihood: a model for the data that depends on the parameters.\n\nFor the regression, we have the model \\(Y_i | \\boldsymbol{\\beta}, \\sigma^2 \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2) \\equiv p(y_i | \\boldsymbol{\\beta}, \\sigma^2) = \\phi(y_i | \\beta_0 + \\beta_1 x_i, \\sigma^2).\\) Note that we are conditioning on the parameters.\nIn vector notation, this is \\(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N_N(X\\boldsymbol{\\beta}, \\sigma^2 I_N).\\)\n\nThe Posterior: a distribution that uses Bayes rule to define distribution of the parameters given the data.\n\nFor the regression model we are able to find an analytic solution to the posterior when we have a non-informative prior \\(p(\\boldsymbol{\\beta}, \\sigma^2) \\propto \\sigma^{-2}\\), or for a conjugate prior \\(p(\\boldsymbol{\\beta} | \\sigma^2) = \\phi(\\beta_0) \\phi(\\beta_1)\\) and \\(p(\\sigma^2) = InvGamma(\\frac{\\nu_0}{2}, \\frac{1}{2} \\nu_0 s_0^2).\\)\nThe joint posterior for the non-informative prior is expressed as\n\n\n\\[\\begin{align*}\n    p(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{y}) &= p(\\boldsymbol{\\beta} | \\sigma^2, \\mathbf{y}) \\times p(\\sigma^2 | \\mathbf{y}) \\\\\n        &= N_2(\\boldsymbol{\\beta} | \\hat{\\boldsymbol{\\beta}}, \\sigma^2 (X'X)^{-1}) \\times Inv-\\chi^2 (\\sigma^2 | N-2, s^2) \\\\\n    \\hat{\\boldsymbol{\\beta}} &= (X'X)^{-1} X'\\mathbf{y} \\\\\n    s^2 &= \\frac{1}{N-2} (\\mathbf{y} - X\\hat{\\boldsymbol{\\beta}})' (\\mathbf{y} - X\\hat{\\boldsymbol{\\beta}})   \n\\end{align*}\\]"
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#some-examples-in-stan",
    "href": "posts/intro-to-stan/intro_to_stan.html#some-examples-in-stan",
    "title": "Getting to know Stan - SWOSC",
    "section": "Some examples in Stan",
    "text": "Some examples in Stan\nWe’ll continue with the single predictor regression model to model NCAA Women’s Basketball team’s total wins by their 3 point field goal percentage from the 2022-2023 season. Data collected from NCAA.\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\n\nncaaw &lt;- readr::read_csv(file = \"Data/NCAAW-freethrows-threes-2022-2023.csv\")\n\n\nRows: 350 Columns: 11\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Team, WL\ndbl (9): G, FT, FTA, FTpct, FG3, FG3A, FG3pct, W, L\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIn the 2022-2023 season there were \\(N = 350\\) teams. The relationship between their wins and three point percentage is displayed in Figure 1.\n\n\nCode\nggplot(ncaaw, aes(FG3pct, W)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(title = \"2022-23 NCAAW Wins by 3pt%\", \n         subtitle = paste0(\"r = \", round(cor(ncaaw$W, ncaaw$FG3pct), 3)),\n         x = \"3pt%\",\n         y = \"Wins\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure 1: Scatter plot of the Total Wins by 3 pt Field Goal %. The OLS regression line is super-imposed in blue.\n\n\n\n\nAs a baseline, we’ll find the maximum likelihood estimates for the regression parameters and variance.\n\n\nCode\nfit_ml &lt;- lm(W ~ FG3pct, data = ncaaw)\n\ncoef(fit_ml)\n\n\n(Intercept)      FG3pct \n  -14.94468     1.00929 \n\n\nCode\nsmry_ml &lt;- summary(fit_ml)\nsmry_ml$sigma^2\n\n\n[1] 34.90616\n\n\n\nNon-informative Prior Regression Model\nFirst we write the Stan code in a separate file. See the Stan User’s Guide Part 1.1 for programming this model without the analytic posteriors.\n\n\nnon-informative-regression.stan\n\n// The input data is two vectors 'y' and 'X' of length 'N'.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  vector[N] x;\n}\n\ntransformed data {\n    matrix[N, 2] X_c = append_col(rep_vector(1, N), x);\n    matrix[2,2] XtX_inv = inverse(X_c' * X_c);\n\n    vector[2] beta_hat = XtX_inv * X_c' * y;\n    vector[N] y_hat = X_C * beta_hat;\n    \n    real&lt;lower=0&gt; s_2 = 1 / (N - 2) * (y - y_hat)' * (y - y_hat);\n}\n\n// The parameters accepted by the model. Our model\n// accepts two parameters 'beta' and 'sigma'.\nparameters {\n  vector beta;\n  real&lt;lower=0&gt; sigma; // Note that this is the variance\n}\n\n// The model to be estimated. We model the output\n// 'y' ~ N(x beta, sigma) by specifying the analytic\n// posterior defined above.\nmodel {\n  beta ~ multi_normal(beta_hat, sigma^2 * XtX_inv);\n  \n  sigma^2 ~ scaled_inv_chi_square(N-2, sqrt(s_2));\n}\n\ngenerated quantities {\n    vector[N] y_ppd;\n    \n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_c[i,] * beta, sigma);\n    }\n}\n\nNext we fit the model using cmdstanr. In this case, I will use 1000 warmup iterations, 1000 sampling iterations, with no thinning (thinning includes only every \\(k\\)th draw), and will refresh the print screen to see progress every 500 iterations. We can run several chains to see if where the chains start dictates any part of the posterior shape or location, and chains can be run in parallel to get more draws &lt;=&gt; better posterior approximation more quickly.\n\n\nCode\ndata_list &lt;- list(\n    N = nrow(ncaaw),\n    y = ncaaw$W,\n    x = ncaaw$FG3pct\n)\n\nfile &lt;- file.path(\"non-informative-regression.stan\")\nnon_inf_model &lt;- cmdstan_model(file)\n\nfit1 &lt;- non_inf_model$sample(\n    data = data_list,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    thin = 1,\n    refresh = 500,\n    chains = 2,\n    show_messages = TRUE,\n    show_exceptions = FALSE\n)\n\n\nRunning MCMC with 2 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.6 seconds.\n\n\nNext we’ll check diagnostics for the sampler. First, we will look at the numeric diagnostic output from the method $diagnostic_summary() which reports if any transitions were divergent, if maximum tree depth was reached, and EBFMI. For this model and data set we don’t see any issues in these summaries.\nNext, we check the traceplots in Figure 2 (a). The MCMC draws can be collected from the fit object using the $draws() method. These plots display the sampled values for each parameter in a line plot. We are looking for a horizontal fuzzy bar. Then we can also look at density plots in Figure 2 (b) which will tell us if the chains reached reasonably similar densities. On both counts, we are in good shape. The plots are created using the bayesplot package.\n\n\nCode\nfit1$diagnostic_summary()\n\n\n$num_divergent\n[1] 0 0\n\n$num_max_treedepth\n[1] 0 0\n\n$ebfmi\n[1] 1.001863 1.101070\n\n\n\n\nCode\nlibrary(bayesplot)\n\n\nThis is bayesplot version 1.10.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\nCode\nmcmc_trace(fit1$draws(variables = c(\"beta\", \"sigma\")))\n\nmcmc_dens_overlay(fit1$draws(variables = c(\"beta\", \"sigma\")))\n\n\n\n\n\n\n\n\n(a) Traceplots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\nFigure 2: Diagnostic plots for the posterior samples. Plots were made using the bayesplot package.\n\n\n\nFinally, after checking that the MCMC chains and diagnostics look satisfactory, we can continue to inference. The summary statistics for the parameters are displayed in Table 1. This are generated by default with the $summary() method. The statistics include the posterior mean (mean), median (median), standard deviation (sd), mean absolute deviation (mad), and lower (q5) and upper bounds (q95) for a 90% credible interval. The statistics rhat, ess_bulk, and ess_tail are additional diagnostic measures that indicate how well the chains are sampling the posterior and how many effective draws we have made. Ideally rhat is very near 1, even 1.01 can be a significant problem. The effective sample sizes should be large/near the number of sampling iterations.\n\n\nCode\nfit1$summary(variables = c(\"beta\", \"sigma\")) |&gt; \n    kableExtra::kbl(booktabs = TRUE, format = \"html\")\n\n\n\n\nTable 1: Summary statistics for the posterior samples for \\(\\beta\\) and \\(\\sigma\\).\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nbeta[1]\n-14.917746\n-14.86870\n2.9209916\n2.9490397\n-19.5959800\n-10.098950\n1.003222\n593.7010\n702.8783\n\n\nbeta[2]\n1.008556\n1.00855\n0.0936737\n0.0947700\n0.8561433\n1.160748\n1.002490\n592.1065\n740.6761\n\n\nsigma\n5.928982\n5.92578\n0.2305663\n0.2281647\n5.5632945\n6.307927\n1.000307\n759.7555\n560.6763\n\n\n\n\n\n\n\n\nWe can also check graphical summaries of this same information such as interval plots for each parameter’s credible intervals or density/area plots. In Figure 3 (a) we have the 50% (thick bar) and 95% (thin bar) credible intervals with the posterior mean displayed as a point. The densities are plotted in ridgelines in Figure 3 (b) with areas shaded underneath to indicate the 50% interval and the width of the density indicates the 95% interval.\n\n\nCode\nmcmc_intervals(fit1$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_areas_ridges(fit1$draws(variables = c(\"beta\", \"sigma\")),\n                  prob_outer = 0.95, prob = 0.5)\n\n\n\n\n\n\n\n\n(a) Interval plots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\) in a ridgeline plot.\n\n\n\n\nFigure 3: Plots for the 50% Credible Interval (inner band) and 95% Credible Interval (outer band) for \\(\\beta\\) and \\(\\sigma\\). Plots were made using the bayesplot package.\n\n\n\nOne additional way to check model fit is to assess posterior predictive checks. To do so we draw samples from the posterior predictive distribution \\(p(y^{new} | y) = \\int p(y^{new} | \\boldsymbol{\\beta}, \\sigma) p(\\boldsymbol{\\beta}, \\sigma | y) d\\boldsymbol{\\beta}d\\sigma\\) by first sampling from the posterior (i.e. the draws in the MCMC chains) and then for each set of draws sampling \\(y^{new}\\) given the corresponding values for \\(x^{new}\\). In Stan this is easily accomplished using the generated quantities block. The generated quantities block generates new samples that we define using the current iteration’s posterior draws of \\(\\beta\\) and \\(\\sigma\\).\ngenerated quantities {\n    // create a vector of N new observations\n    vector[N] y_ppd; \n    \n    // for each observation, sample from the regression likelihod\n    // using the posterior draws\n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_c[i,] * beta, sigma);\n    }\n}\nWe collect the PPD draws from the fit object using the draws method. From Figure 4 we can see that while the predictive densities are centered in the correct location, the variances are far too large.\n\n\nCode\nlibrary(posterior)\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following object is masked from 'package:bayesplot':\n\n    rhat\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nCode\nfit1$draws(variables = \"y_ppd\") |&gt; \n    as_draws_df() |&gt;\n    as.matrix() -&gt; y_ppd\n\n\nppc_dens_overlay(ncaaw$W,\n                 y_ppd[1:50, 1:350]) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins\",\n         x = \"Wins\")\n\nppc_intervals(ncaaw$W,\n                 y_ppd[1:50, 1:350],\n                 x = ncaaw$FG3pct) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins by 3pt%\",\n         x = \"3pt%\",\n         y = \"Wins\")\n\n\n\n\n\n\n\n\n(a) PPD densities for the wins given 3pt%.\n\n\n\n\n\n\n\n(b) PPD intervals for the wins plotted by 3pt%.\n\n\n\n\nFigure 4: Posterior Predictive Check plots from bayesplot.\n\n\n\n\n\nConjugate Prior Regression Model\nNext, we’ll implement the regression model with conjugate priors. Conjugacy refers to the situation where the prior and posterior distribution are from the same family. We’ll start by re-defining our model.1\n\nConjugate prior: \\(p(\\boldsymbol{\\beta}, \\sigma^2) = p(\\boldsymbol{\\beta} | \\sigma^2) p(\\sigma^2)\\)\n\n\\(\\boldsymbol{\\beta} | \\sigma^2 ~ N_2(\\boldsymbol{\\beta}_0, \\sigma^2 \\Lambda_0^{-1})\\) where \\(\\boldsymbol{\\beta}_0 \\in \\mathbb{R}^2\\) is a vector of prior coefficients, we’ll set it to zero, and \\(\\Lambda_0\\) is a \\(2\\times2\\) prior correlation matrix. We will set \\(\\Lambda_0 = 10 I_2\\) to get a weakly informative prior that is equivalent to ridge regression.\n\\(\\sigma^2 \\sim InvGamma(\\frac{\\nu_0}{2}, \\frac{1}{2} \\nu_0 s_0^2)\\) where \\(\\nu_0\\) is a prior sample size and \\(s_0\\) is the prior standard deviation. We’ll set these to \\(\\nu_0 = 1\\) and \\(s_0^2 = 47\\) which is approximately the sample variance of the NCAA women’s basketball teams’ wins.\nThe parameters \\(\\boldsymbol{\\beta}_0, \\Lambda_0, \\nu_0, s_0^2\\) that define the prior are referred to as hyperparameters. We will set them before running the model, although they could also be modeled if we wanted.\n\nThe (Data) Likelihood: the same as before, \\(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N_N(X\\boldsymbol{\\beta}, \\sigma^2 I_N).\\)\nPosterior:\n\n\\(\\boldsymbol{\\beta} | \\sigma^2, y \\sim N_2(\\boldsymbol{\\beta}_N, \\sigma^2 \\Lambda_N^{-1})\\) where \\(\\boldsymbol{\\beta}_N = \\Lambda_N^{-1}(\\mathbf{X}'\\mathbf{X} \\hat{\\boldsymbol{\\beta}} + \\Lambda_0 \\boldsymbol{\\beta}_0)\\) and \\(\\Lambda_N = (\\mathbf{X}'\\mathbf{X} + \\Lambda_0).\\)\n\\(\\sigma^2 | y) \\sim InvGamma(\\sigma^2 | \\frac{\\nu_0 + N}{2}, \\frac{1}{2} \\nu_0 s_0^2 + \\frac{1}{2}(\\mathbf{y}'\\mathbf{y} + \\boldsymbol{\\beta}_0'\\Lambda_0 \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_N' \\Lambda_N \\boldsymbol{\\beta}_N)).\\)\n\n\nWe could again program this model using the analytic posterior. Instead, we’ll program it only through the priors and likelihood and let Stan approximate the posterior. I will also allow the model to include more than one predictor so that \\(\\mathbf{X}\\) is a \\(N \\times (K+1)\\) matrix augmented with a column of ones.\n\n\nconjugate-regression.stan\n\n\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; K;\n  vector[N] y;\n  matrix[N, K] X;\n  \n  // hyperparameters\n  real beta_0;\n  real&lt;lower=0&gt; lambda_0;\n  real&lt;lower=0&gt; nu_0;\n  real&lt;lower=0&gt; s_02;\n}\n\ntransformed data {\n    matrix[N, K+1] X_mat = append_col(rep_vector(1, N), X);\n    vector[K+1] beta_0_vec = rep_vector(beta_0, K+1);\n    matrix[K+1, K+1] Lambda_0 = lambda_0 * identity_matrix(K+1);\n}\n\n// The parameters accepted by the model. Our model\n// accepts two parameters 'mu' and 'sigma'.\nparameters {\n  vector[K+1] beta;\n  real&lt;lower=0&gt; sigma2;\n}\n\n// The model to be estimated. We model the output\n// 'y' to be normally distributed with mean 'mu'\n// and standard deviation 'sigma'.\nmodel {\n  beta ~ multi_normal(beta_0_vec, sigma2 * Lambda_0);\n  sigma2 ~ scaled_inv_chi_square(nu_0, s_02);\n  \n  y ~ normal(X_mat * beta, sqrt(sigma2));\n}\n\ngenerated quantities {\n    real sigma = sqrt(sigma2);\n    vector[N] y_ppd;\n    \n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_mat[i,] * beta, sqrt(sigma2));\n    }\n}\n\n\n\nCode\ndata_list2 &lt;- list(\n    N = nrow(ncaaw),\n    K = 1,\n    y = ncaaw$W,\n    X = as.matrix(ncaaw$FG3pct, nrow = nrow(ncaaw)),\n    \n    # hyperparameters\n    beta_0 = 0,\n    lambda_0 = 10,\n    nu_0 = 1,\n    s_02 = 47\n)\n\nfile2 &lt;- file.path(\"conjugate-regression.stan\")\nconj_model &lt;- cmdstan_model(file2)\n\nfit2 &lt;- conj_model$sample(\n    data = data_list2,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    thin = 1,\n    refresh = 500,\n    chains = 2,\n    show_messages = TRUE,\n    show_exceptions = FALSE\n)\n\n\nRunning MCMC with 2 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.4 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.3 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.9 seconds.\n\n\n\n\nCode\nfit2$diagnostic_summary()\n\n\n$num_divergent\n[1] 0 0\n\n$num_max_treedepth\n[1] 0 0\n\n$ebfmi\n[1] 1.0206289 0.9623406\n\n\n\n\nCode\nlibrary(bayesplot)\n\nmcmc_trace(fit2$draws(variables = c(\"beta\", \"sigma\")))\n\nmcmc_dens_overlay(fit2$draws(variables = c(\"beta\", \"sigma\")))\n\n\n\n\n\n\n\n\n(a) Traceplots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\nFigure 5: Diagnostic plots for the posterior samples. Plots were made using the bayesplot package.\n\n\n\nAgain the MCMC chains and diagnostics look satisfactory. The summary statistics for the parameters are displayed in Table 2.\n\n\nCode\nfit2$summary(variables = c(\"beta\", \"sigma\")) |&gt; \n    kableExtra::kbl(booktabs = TRUE, format = \"html\")\n\n\n\n\nTable 2: Summary statistics for the posterior samples for \\(\\beta\\) and \\(\\sigma\\).\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nbeta[1]\n-14.4943842\n-14.3745500\n3.1037441\n3.1268034\n-19.6791600\n-9.335630\n1.001786\n584.3318\n684.0491\n\n\nbeta[2]\n0.9948555\n0.9921695\n0.0997815\n0.1009139\n0.8260818\n1.160146\n1.001759\n579.1196\n649.1763\n\n\nsigma\n6.4180363\n6.4040900\n0.2499544\n0.2355036\n6.0326720\n6.856377\n1.004438\n849.0158\n715.7885\n\n\n\n\n\n\n\n\nIn Figure 6 (a) we have the 50% and 95% CIs with the posterior mean displayed as a point. The densities are plotted in ridgelines in Figure 6 (b).\n\n\nCode\nmcmc_intervals(fit2$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_areas_ridges(fit2$draws(variables = c(\"beta\", \"sigma\")),\n                  prob_outer = 0.95, prob = 0.5)\n\n\n\n\n\n\n\n\n(a) Interval plots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\) in a ridgeline plot.\n\n\n\n\nFigure 6: Plots for the 50% Credible Interval (inner band) and 95% Credible Interval (outer band) for \\(\\beta\\) and \\(\\sigma\\). Plots were made using the bayesplot package.\n\n\n\nWe collect the PPD draws from the fit object using the draws method. From Figure 7 we can see that while the predictive densities match pretty well and the intervals are centered on the OLS line of best fit.\n\n\nCode\nlibrary(posterior)\n\nfit2$draws(variables = \"y_ppd\") |&gt; \n    as_draws_df() |&gt;\n    as.matrix() -&gt; y_ppd\n\n\nppc_dens_overlay(ncaaw$W,\n                 y_ppd[1:50, 1:350]) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins\",\n         x = \"Wins\")\n\nppc_intervals(ncaaw$W,\n                 y_ppd[1:50, 1:350],\n                 x = ncaaw$FG3pct) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins by 3pt%\",\n         x = \"3pt%\",\n         y = \"Wins\")\n\n\n\n\n\n\n\n\n(a) PPD densities for the wins given 3pt%.\n\n\n\n\n\n\n\n(b) PPD intervals for the wins plotted by 3pt%.\n\n\n\n\nFigure 7: Posterior Predictive Check plots from bayesplot.\n\n\n\n\n\nBonus: Regression Modeling with Incomplete Data\nAs a bonus section, we’ll use the brms package to fit a regression model where we have incomplete predictor observations. Incomplete data analysis ranges from complete case analysis (incomplete cases are dropped) and mean imputation to multiple imputation, joint modeling, and EM algorithm (Schafer and Graham 2002).2 We’re going to use mice (Buuren and Groothuis-Oudshoorn 2010) and brms (Bürkner 2018) to demonstrate the imputation and fitting Bayesian regression models with a convenient front-end that writes the Stan code for us.\nIn our case, we are going to use junior year scoring (points per game) to predict senior year scoring for women’s college basketball players from 2020-21 to the 2022-23 seasons. The data set only contains players who played in at least 75% of games each season, so partial seasons due to injury or being a bench player are excluded. Players who only have a junior season are excluded from the analysis.\n\n\nCode\nncaaw_i &lt;- read.csv(\"Data/ncaaw-individuals.csv\", header = TRUE)\nhead(ncaaw_i)\n\n\n             Name Pos_jr Pos_sr G_jr G_sr PPG_jr PPG_sr Cl_jr\n1     A'Jah Davis      F      F   29   32   16.6   16.2   Jr.\n2 Abby Brockmeyer      F      F   NA   31     NA   16.3   Jr.\n3       Abby Feit      F      F   29   28   15.1   15.5   Jr.\n4     Abby Meyers      G      G   NA   30     NA   17.9   Jr.\n5     Abby Meyers      G      G   NA   35     NA   14.3   Jr.\n6   Adriana Shipp      G      G   NA   30     NA   13.9   Jr.\n\n\nOur imputation model will be univariate linear regression that use all other variables as predictors. For example, imputing \\(PPG_{jr}\\) will be done by regressing on \\(PPG_{sr}, G_{jr}, G_{sr}\\). \\(PPG_{jr}\\) and \\(G_{jr}\\) are incomplete for \\(n_{mis} = 176\\) players while \\(n_{obs} = 98\\) players have stats from both years. This missing data pattern is displayed in Figure 8.\n\n\nCode\nlibrary(mice)\n\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n\nCode\nm_pat &lt;- md.pattern(ncaaw_i, plot = TRUE)\n\n\n\n\n\nFigure 8: Missing data patterns for the NCAA women’s basketball players from 2020-2023 who played in their junior and senior year. The red boxes correspond to missing values, so there are 176 players who recorded full senior seasons (played in &gt;75% of total games) but missing or shortened junior seasons.\n\n\n\n\n\n\nCode\nggplot(ncaaw_i, aes(PPG_jr, PPG_sr, color = G_jr)) +\n    geom_point() +\n    scale_color_viridis_c(name = \"G - Jr\") +\n    labs(x = \"PPG - Jr\",\n         y = \"PPG - Sr\") +\n    theme_bw()\n\n\n\n\n\nFigure 9: Points per game (PPG) from Junior and Senior seasons.\n\n\n\n\n\nMultiple Imputation with mice\nFirst, we’ll try out imputing before model fitting using mice. MICE stands for Multiple Imputation by Chained Equations and is procedure that creates a set of \\(M\\) completed data sets from an incomplete data set. Multiple Imputation is a three stage procedure:\n\nEach incomplete variable is imputed with posterior predictive draws from a regression model with all other variables as predictors. The procedure iterates through the incomplete variables several times to converge to the posterior predictive distribution of the missing data given the observed.\nThese completed data sets are then analyzed individually with a standard complete data method.\nResults from each analysis are combined. Typically this is done with Rubin’s rules (Rubin 1987), but brms follows the advice of Zhou and Reiter (2010) and simply stacks the posterior draw matrices from each fitted model.\n\n\n\nCode\nlibrary(brms)\n\nimps &lt;- mice(ncaaw_i, m = 10, method = \"norm\", maxit = 10, printFlag = FALSE)\n\nfit_brm_mice &lt;- brm_multiple(PPG_sr ~ G_jr * PPG_jr, data = imps, chains = 2,\n                             refresh = 0)\nsummary(fit_brm_mice)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: PPG_sr ~ G_jr * PPG_jr \n   Data: imps (Number of observations: 274) \n  Draws: 20 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 20000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      17.86      4.82     8.80    27.62 1.14       94      281\nG_jr           -0.25      0.19    -0.65     0.11 1.17       80      206\nPPG_jr         -0.10      0.29    -0.69     0.43 1.18       75      232\nG_jr:PPG_jr     0.02      0.01    -0.01     0.04 1.21       67      170\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.21      0.10     2.02     2.41 1.04      396     3350\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nCode\nplot(brms::conditional_effects(fit_brm_mice, \"PPG_jr:G_jr\", resp = \"PPGsr\")) +\n    coord_cartesian(xlim = c(15, 30))\n\n\nNULL\n\n\n\n\n\nFigure 10: The estimated conditional effects of PPG as a junior and junior-year Games played on PPG as a senior.\n\n\n\n\n\n\nImputation During Model Fitting\nImputation during model fitting takes a different approach. Imputations are made for each incomplete variable using a different conditional model for each variable. This approach differs from MI and MICE in two key ways: (i) the model is only fit once since the imputation model is part of the analysis model, (ii) the model must be constructed uniquely for each analysis scenario whereas MI completed data sets can be re-used with different analyses.\n\n\nCode\nbform &lt;- bf(PPG_sr | mi() ~ mi(G_jr) * mi(PPG_jr)) +\n    bf(PPG_jr | mi() ~ G_sr + PPG_sr) +\n    bf(G_jr | mi() ~ G_sr + PPG_sr) + set_rescor(FALSE)\n\nfit_brm_mi &lt;- brm(bform, data = ncaaw_i, \n                  refresh = 500, iter = 2000, thin = 1,\n                  backend = \"cmdstanr\", \n                  control = list(adapt_delta = 0.8, \n                                 max_depth = 10,\n                                show_exceptions = FALSE),\n                  chains = 2,\n                  cores = 2)\n\nsummary(fit_brm_mi)\n\n\n Family: MV(gaussian, gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: PPG_sr | mi() ~ mi(G_jr) * mi(PPG_jr) \n         PPG_jr | mi() ~ G_sr + PPG_sr \n         G_jr | mi() ~ G_sr + PPG_sr \n   Data: ncaaw_i (Number of observations: 274) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nPPGsr_Intercept          15.99      2.27    11.31    20.63 1.00     1214\nPPGjr_Intercept           5.88      2.34     1.29    10.37 1.00     1999\nGjr_Intercept             2.71      6.17    -9.23    14.63 1.00     1356\nPPGjr_G_sr               -0.04      0.07    -0.17     0.09 1.00     2345\nPPGjr_PPG_sr              0.71      0.08     0.55     0.87 1.00     1359\nGjr_G_sr                  0.56      0.18     0.22     0.90 1.00     1549\nGjr_PPG_sr                0.28      0.23    -0.17     0.72 1.00      908\nPPGsr_miG_jr             -0.29      0.09    -0.48    -0.10 1.00     1084\nPPGsr_miPPG_jr           -0.03      0.14    -0.31     0.27 1.00     1083\nPPGsr_miG_jr:miPPG_jr     0.02      0.01     0.01     0.03 1.00     1076\n                      Tail_ESS\nPPGsr_Intercept           1250\nPPGjr_Intercept           1730\nGjr_Intercept             1301\nPPGjr_G_sr                1370\nPPGjr_PPG_sr              1209\nGjr_G_sr                  1431\nGjr_PPG_sr                 951\nPPGsr_miG_jr              1249\nPPGsr_miPPG_jr            1135\nPPGsr_miG_jr:miPPG_jr     1256\n\nFamily Specific Parameters: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_PPGsr     1.86      0.09     1.70     2.05 1.00     2333     1705\nsigma_PPGjr     2.30      0.16     2.02     2.64 1.00      908     1053\nsigma_Gjr       5.33      0.40     4.63     6.19 1.01      482      961\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nbrms is built on Stan, so we can also take a look at the traceplots of the samples in Figure 11.\n\n\nCode\nplot(fit_brm_mi, variable = c(\"b_PPGsr\", \"bsp_\"), regex = TRUE, ask = FALSE, N = 3)\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 11: Traceplots of brms analysis model parameters.\n\n\n\n\n\nCode\nplot(fit_brm_mi, variable = c(\"b_PPGjr\", \"b_Gjr\"), regex = TRUE, ask = FALSE, N = 3)\n\n\n\n\n\n\n\n\n(a) \\(PPG_{jr}\\) imputation model parameters\n\n\n\n\n\n\n\n\n\n(b) \\(G_{jr}\\) imputation model parameters\n\n\n\n\nFigure 12: Traceplots of brms imputation model parameters.\n\n\n\n\n\nCode\nplot(brms::conditional_effects(fit_brm_mi, \"PPG_jr:G_jr\", resp = \"PPGsr\"))\n\n\n\n\n\nFigure 13: The estimated conditional effects of PPG as a junior and junior-year Games played on PPG as a senior."
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#some-other-useful-resources-for-stan",
    "href": "posts/intro-to-stan/intro_to_stan.html#some-other-useful-resources-for-stan",
    "title": "Getting to know Stan - SWOSC",
    "section": "Some Other Useful Resources for Stan",
    "text": "Some Other Useful Resources for Stan\nFirst, here’s the three essential guides for using Stan:\n\nStan Function Guide - reference for all the built-in functions and distributions as well as guides for writing custom functions and distributions\nStan User’s Guide - reference for example models, how to build efficient models, and some inference techniques\nStan Reference Manual - reference for programming in Stan with a focus on how the language works\n\nHere are some other useful packages to use for Bayesian data analysis with Stan (or other packages). We used some of these in this tutorial!\n\nbrms: Bayesian regression models using Stan\nposterior: Useful for working with Stan output\nbayesplot: ggplot2-based plotting functions for MCMC draws designed work well with Stan\nloo: Leave-one-out cross validation for model checking and selection that works with the log-posterior. Works best with rstanarm but can work with cmdstanr too.\n\nHere’s a list of useful resources for debugging issues with divergences, hitting maximum tree-depth, low EBFMI, and understanding diagnostics:\n\nStan’s Guide to Runtime warnings and convergence problems\nPrior Choices and Selection\nConvergence Diagnostics for MCMC\nOfficial Stan Forum"
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#footnotes",
    "href": "posts/intro-to-stan/intro_to_stan.html#footnotes",
    "title": "Getting to know Stan - SWOSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee wikipedia for more details and derivations: https://en.wikipedia.org/wiki/Bayesian_linear_regression↩︎\nSee White, Royston, and Wood (2011) for more details on incomplete data analysis.↩︎"
  }
]